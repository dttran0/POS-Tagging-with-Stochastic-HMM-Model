{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import string\n",
    "from nltk.tag import hmm\n",
    "from nltk.corpus import treebank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\trand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\trand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\trand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\trand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\trand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') # download for lemmatization\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMM + Viterbi for Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get each sentence and tokenize as word \n",
    "# tokenized_sentences = []\n",
    "# for sentence in sentences:\n",
    "#     words = word_tokenize(sentence)\n",
    "#     words = [word for word in words if word not in string.punctuation]\n",
    "#     tokenized_sentences.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_punctuation(tagged_sentences):\n",
    "    filtered_sentences = []\n",
    "    for sentence in tagged_sentences:\n",
    "        filtered_sentence = [(word, tag) for word, tag in sentence if word not in string.punctuation]\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the treebank corpus\n",
    "tagged_sentences = treebank.tagged_sents(tagset='universal')\n",
    "tagged_sentences = filter_punctuation(tagged_sentences)\n",
    "\n",
    "# Split into training and testing data\n",
    "train_data = tagged_sentences[:3000]\n",
    "test_data = tagged_sentences[3000:]\n",
    "\n",
    "# Create dictionaries to hold emission and transition probabilities\n",
    "emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "tag_counts = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count emissions and transitions\n",
    "for sentence in train_data:\n",
    "    previous_tag = '<s>'\n",
    "    for word, tag in sentence:\n",
    "        emission_counts[tag][word.lower()] += 1\n",
    "        transition_counts[previous_tag][tag] += 1\n",
    "        tag_counts[tag] += 1\n",
    "        previous_tag = tag\n",
    "    transition_counts[previous_tag]['</s>'] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate emission probabilities: for the observable states\n",
    "emission_prob = defaultdict(lambda: defaultdict(float))\n",
    "for tag, words in emission_counts.items():\n",
    "    total_count = float(tag_counts[tag])\n",
    "    for word, count in words.items():\n",
    "        emission_prob[tag][word] = count / total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate transition probabilities: a dictionary that tells the probability of getting a particular tag as \n",
    "#the next POS tag given that which tag had occured previously.\n",
    "transition_prob = defaultdict(lambda: defaultdict(float))\n",
    "for prev_tag, next_tags in transition_counts.items():\n",
    "    total_count = float(sum(next_tags.values()))\n",
    "    for next_tag, count in next_tags.items():\n",
    "        transition_prob[prev_tag][next_tag] = count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in emission_prob.items():\n",
    "#     print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def viterbi(sentence, transition_prob, emission_prob, tag_counts):\n",
    "#     states = list(tag_counts.keys())\n",
    "#     V = [{}]\n",
    "#     path = {}\n",
    "\n",
    "#     # Initialize base cases (t == 0)\n",
    "#     for state in states:\n",
    "#         V[0][state] = transition_prob['<s>'][state] * emission_prob[state].get(sentence[0], 1e-6)\n",
    "#         path[state] = [state]\n",
    "\n",
    "#     # Run Viterbi for t > 0\n",
    "#     for t in range(1, len(sentence)):\n",
    "#         V.append({})\n",
    "#         newpath = {}\n",
    "\n",
    "#         for state in states:\n",
    "#             (prob, state_max) = max((V[t-1][y0] * transition_prob[y0].get(state, 1e-6) * emission_prob[state].get(sentence[t], 1e-6), y0) for y0 in states)\n",
    "#             V[t][state] = prob\n",
    "#             newpath[state] = path[state_max] + [state]\n",
    "\n",
    "#         path = newpath\n",
    "\n",
    "#     # Choose the best final state\n",
    "#     (prob, state_max) = max((V[len(sentence) - 1][state], state) for state in states)\n",
    "#     return path[state_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi algorithm\n",
    "def viterbi(observation_seqs, transition_prob, emission_prob, tag_counts):\n",
    "    states = list(tag_counts.keys())\n",
    "    num_states = len(states)\n",
    "    num_obs = len(observation_seqs)\n",
    "    \n",
    "    # Initialize the probability matrix and the backpointer matrix\n",
    "    prob_matrix = np.zeros((num_states, num_obs))\n",
    "    backtrack = np.zeros((num_states, num_obs), dtype=int)\n",
    "    \n",
    "    # Initial probabilities\n",
    "    initial_states = np.array([transition_prob['<s>'][state] for state in states])\n",
    "    \n",
    "    # Populate the initial column of the probability matrix\n",
    "    for state_index, state in enumerate(states):\n",
    "        prob_matrix[state_index, 0] = initial_states[state_index] * emission_prob[state].get(observation_seqs[0], 1e-6)\n",
    "    \n",
    "    # Populate the probability matrix for t > 0\n",
    "    for t in range(1, num_obs):\n",
    "        for state_index, state in enumerate(states):\n",
    "            max_prob, max_state = max(\n",
    "                (prob_matrix[prev_state_index, t-1] * transition_prob[prev_state][state] * emission_prob[state].get(observation_seqs[t], 1e-6), prev_state_index)\n",
    "                for prev_state_index, prev_state in enumerate(states)\n",
    "            )\n",
    "            prob_matrix[state_index, t] = max_prob\n",
    "            backtrack[state_index, t] = max_state\n",
    "    \n",
    "    # Find the most probable state sequence\n",
    "    optimal_path = np.zeros(num_obs, dtype=int)\n",
    "    optimal_path[-1] = np.argmax(prob_matrix[:, -1])\n",
    "    \n",
    "    for t in range(num_obs - 2, -1, -1):\n",
    "        optimal_path[t] = backtrack[optimal_path[t + 1], t + 1]\n",
    "    \n",
    "    # Convert indices back to state names\n",
    "    optimal_tags = [states[state_index] for state_index in optimal_path]\n",
    "    \n",
    "    return optimal_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "words_list = []\n",
    "predicted_tags_list = []\n",
    "gold_tag_list = []\n",
    "def evaluate_hmm(test_data, transition_prob, emission_prob, tag_counts):\n",
    "    correct = total = 0\n",
    "    for sentence in test_data:\n",
    "        words, gold_tags = zip(*sentence)\n",
    "        words = [word.lower() for word in words]\n",
    "        predicted_tags = viterbi(words, transition_prob, emission_prob, tag_counts)\n",
    "        correct += sum(p == g for p, g in zip(predicted_tags, gold_tags))\n",
    "        total += len(gold_tags)\n",
    "        # Print the comparison for each sentence\n",
    "        words_list.append(' '.join(words))\n",
    "        predicted_tags_list.append(predicted_tags)\n",
    "        gold_tag_list.append(gold_tags)\n",
    "        # print(f\"Sentence: {' '.join(words)}\")\n",
    "        # print(f\"Predicted tags: {predicted_tags}\")\n",
    "        # print(f\"Actual tags:    {gold_tags}\")\n",
    "        # print()\n",
    "    #showing the first 5 performance \n",
    "    for i in range(5):\n",
    "        print(f\"Sentence: {words_list[i]}\")\n",
    "        print(f\"Predicted tags: {predicted_tags_list[i]}\")\n",
    "        print(f\"Actual tags:    {gold_tag_list[i]}\")\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DET'), ('ceremony', 'NOUN'), ('was', 'VERB'), ('held', 'VERB'), ('at', 'ADP'), ('westminster', 'NOUN'), ('abbey', 'NOUN'), ('.', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = word_tokenize(\"The ceremony was held at Westminster Abbey.\".lower())\n",
    "predicted_tags = viterbi(test_sentence, transition_prob, emission_prob, tag_counts)\n",
    "print(list(zip(test_sentence, predicted_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: at tokyo the nikkei index of 225 selected issues which *t*-1 gained 132 points tuesday added 14.99 points to 35564.43\n",
      "Predicted tags: ['ADP', 'NOUN', 'DET', 'NOUN', 'NOUN', 'ADP', 'NUM', 'NOUN', 'NOUN', 'DET', 'X', 'VERB', 'DET', 'NOUN', 'NOUN', 'VERB', 'DET', 'NOUN', 'PRT', 'VERB']\n",
      "Actual tags:    ('ADP', 'NOUN', 'DET', 'NOUN', 'NOUN', 'ADP', 'NUM', 'VERB', 'NOUN', 'DET', 'X', 'VERB', 'NUM', 'NOUN', 'NOUN', 'VERB', 'NUM', 'NOUN', 'PRT', 'NUM')\n",
      "Sentence: in early trading in tokyo thursday the nikkei index fell 63.79 points to 35500.64\n",
      "Predicted tags: ['ADP', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'DET', 'NOUN', 'NOUN', 'VERB', 'DET', 'NOUN', 'PRT', 'VERB']\n",
      "Actual tags:    ('ADP', 'ADV', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'DET', 'NOUN', 'NOUN', 'VERB', 'NUM', 'NOUN', 'PRT', 'NUM')\n",
      "Sentence: wednesday 's volume on the first section was estimated *-1 at 900 million shares in line with tuesday 's 909 million\n",
      "Predicted tags: ['NOUN', 'PRT', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'X', 'ADP', 'NUM', 'NUM', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NOUN', 'PRT', 'NUM', 'NUM']\n",
      "Actual tags:    ('NOUN', 'PRT', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'VERB', 'VERB', 'X', 'ADP', 'NUM', 'NUM', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NOUN', 'PRT', 'NUM', 'NUM')\n",
      "Sentence: declining issues slightly outnumbered advancing issues 454 to 451\n",
      "Predicted tags: ['VERB', 'NOUN', 'ADV', 'VERB', 'VERB', 'NOUN', 'NOUN', 'PRT', 'VERB']\n",
      "Actual tags:    ('VERB', 'NOUN', 'ADV', 'VERB', 'VERB', 'NOUN', 'NUM', 'PRT', 'NUM')\n",
      "Sentence: investors switched trading focus quickly as they did tuesday *-1 reflecting uncertainty about long-term commitments to any issue or sector traders said 0 *t*-2\n",
      "Predicted tags: ['NOUN', 'VERB', 'NOUN', 'VERB', 'ADV', 'ADP', 'PRON', 'VERB', 'NOUN', 'X', 'VERB', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'PRT', 'DET', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'VERB', 'X', 'X']\n",
      "Actual tags:    ('NOUN', 'VERB', 'NOUN', 'NOUN', 'ADV', 'ADP', 'PRON', 'VERB', 'NOUN', 'X', 'VERB', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'PRT', 'DET', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'VERB', 'X', 'X')\n"
     ]
    }
   ],
   "source": [
    "hmm_accuracy = evaluate_hmm(test_data, transition_prob, emission_prob, tag_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM Accuracy with Viterbi: 0.9111\n"
     ]
    }
   ],
   "source": [
    "print(f'HMM Accuracy with Viterbi: {hmm_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs_178",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
