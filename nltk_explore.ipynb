{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import brown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\trand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\trand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\trand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') # download for lemmatization\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\trand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\trand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry. The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066. Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace. Here's how the day of splendour and formality, which featured customs dating back more than 1,000 years, unfolded.\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry. The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066. Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace. Here's how the day of splendour and formality, which featured customs dating back more than 1,000 years, unfolded.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry.',\n",
       " 'The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066.',\n",
       " 'Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace.',\n",
       " \"Here's how the day of splendour and formality, which featured customs dating back more than 1,000 years, unfolded.\"]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Millions', 'of', 'people', 'across', 'the', 'UK', 'and', 'beyond', 'have', 'celebrated', 'the', 'coronation', 'of', 'King', 'Charles', 'III', '-', 'a', 'symbolic', 'ceremony', 'combining', 'a', 'religious', 'service', 'and', 'pageantry', '.']\n"
     ]
    }
   ],
   "source": [
    "#tokenize the sentence in each separate word \n",
    "words = word_tokenize(sentences[0])\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#check out possible stop word in English\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Millions', 'NNS'),\n",
       " ('people', 'NNS'),\n",
       " ('across', 'IN'),\n",
       " ('UK', 'NNP'),\n",
       " ('beyond', 'IN'),\n",
       " ('celebrated', 'VBN'),\n",
       " ('coronation', 'NN'),\n",
       " ('King', 'NNP'),\n",
       " ('Charles', 'NNP'),\n",
       " ('III', 'NNP'),\n",
       " ('symbolic', 'JJ'),\n",
       " ('ceremony', 'NN'),\n",
       " ('combining', 'VBG'),\n",
       " ('religious', 'JJ'),\n",
       " ('service', 'NN'),\n",
       " ('pageantry', 'NN')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print out the stop word\n",
    "\n",
    "# Remove stop words\n",
    "words = [w for w in words if w not in stopwords.words(\"english\") and w not in string.punctuation]\n",
    "token_tag = pos_tag(words)\n",
    "\n",
    "token_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate in HMM model for POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import hmm\n",
    "from nltk.corpus import treebank\n",
    "#we will use treebank --> a widely nlp corpus dataset in ntlk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter punctuation from tagged sentences\n",
    "def filter_punctuation(tagged_sentences):\n",
    "    filtered_sentences = []\n",
    "    for sentence in tagged_sentences:\n",
    "        filtered_sentence = [(word, tag) for word, tag in sentence if word not in string.punctuation]\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "    return filtered_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = treebank.tagged_sents(tagset='universal')\n",
    "tagged_sentences = filter_punctuation(tagged_sentences)\n",
    "# Split into training and testing data\n",
    "#train_elem = int(len(tagged_sentences) *0.8)\n",
    "#print(train_elem)\n",
    "train_data = tagged_sentences[:3000]\n",
    "test_data = tagged_sentences[3000:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the HMM POS tagger\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = trainer.train(train_data)\n",
    "\n",
    "# Tokenize each sentence into words\n",
    "# tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "# print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trand\\anaconda3\\envs\\cs_178\\lib\\site-packages\\nltk\\tag\\hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "c:\\Users\\trand\\anaconda3\\envs\\cs_178\\lib\\site-packages\\nltk\\tag\\hmm.py:364: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: at tokyo the nikkei index of 225 selected issues which *t*-1 gained 132 points tuesday added 14.99 points to 35564.43\n",
      "Predicted tags: ['ADP', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN']\n",
      "Actual tags:    ('ADP', 'NOUN', 'DET', 'NOUN', 'NOUN', 'ADP', 'NUM', 'VERB', 'NOUN', 'DET', 'X', 'VERB', 'NUM', 'NOUN', 'NOUN', 'VERB', 'NUM', 'NOUN', 'PRT', 'NUM')\n",
      "Sentence: in early trading in tokyo thursday the nikkei index fell 63.79 points to 35500.64\n",
      "Predicted tags: ['ADP', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN']\n",
      "Actual tags:    ('ADP', 'ADV', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'DET', 'NOUN', 'NOUN', 'VERB', 'NUM', 'NOUN', 'PRT', 'NUM')\n",
      "Sentence: wednesday 's volume on the first section was estimated *-1 at 900 million shares in line with tuesday 's 909 million\n",
      "Predicted tags: ['NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN']\n",
      "Actual tags:    ('NOUN', 'PRT', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'VERB', 'VERB', 'X', 'ADP', 'NUM', 'NUM', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NOUN', 'PRT', 'NUM', 'NUM')\n",
      "Sentence: declining issues slightly outnumbered advancing issues 454 to 451\n",
      "Predicted tags: ['VERB', 'NOUN', 'ADV', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN']\n",
      "Actual tags:    ('VERB', 'NOUN', 'ADV', 'VERB', 'VERB', 'NOUN', 'NUM', 'PRT', 'NUM')\n",
      "Sentence: investors switched trading focus quickly as they did tuesday *-1 reflecting uncertainty about long-term commitments to any issue or sector traders said 0 *t*-2\n",
      "Predicted tags: ['NOUN', 'VERB', 'NOUN', 'VERB', 'ADV', 'ADP', 'PRON', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN']\n",
      "Actual tags:    ('NOUN', 'VERB', 'NOUN', 'NOUN', 'ADV', 'ADP', 'PRON', 'VERB', 'NOUN', 'X', 'VERB', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'PRT', 'DET', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'VERB', 'X', 'X')\n",
      "Accuracy: 0.4668\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the HMM model on the test data\n",
    "words_list = []\n",
    "predicted_tags_list = []\n",
    "gold_tag_list = []\n",
    "def evaluate_hmm_tagger(test_data, hmm_tagger):\n",
    "    correct = total = 0\n",
    "    for sentence in test_data:\n",
    "        words, gold_tags = zip(*sentence)\n",
    "        words = [word.lower() for word in words if word not in string.punctuation]\n",
    "        predicted_tags = [tag for word, tag in hmm_tagger.tag(words)]\n",
    "        correct += sum(p == g for p, g in zip(predicted_tags, gold_tags))\n",
    "        total += len(gold_tags)\n",
    "\n",
    "        words_list.append(' '.join(words))\n",
    "        predicted_tags_list.append(predicted_tags)\n",
    "        gold_tag_list.append(gold_tags)\n",
    "       \n",
    "    #showing the first 5 performance \n",
    "    for i in range(5):\n",
    "        print(f\"Sentence: {words_list[i]}\")\n",
    "        print(f\"Predicted tags: {predicted_tags_list[i]}\")\n",
    "        print(f\"Actual tags:    {gold_tag_list[i]}\")\n",
    "    return correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: at tokyo the nikkei index of 225 selected issues which *t*-1 gained 132 points tuesday added 14.99 points to 35564.43\n",
      "Predicted tags: ['ADP', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN']\n",
      "Actual tags:    ('ADP', 'NOUN', 'DET', 'NOUN', 'NOUN', 'ADP', 'NUM', 'VERB', 'NOUN', 'DET', 'X', 'VERB', 'NUM', 'NOUN', 'NOUN', 'VERB', 'NUM', 'NOUN', 'PRT', 'NUM')\n",
      "Sentence: in early trading in tokyo thursday the nikkei index fell 63.79 points to 35500.64\n",
      "Predicted tags: ['ADP', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN']\n",
      "Actual tags:    ('ADP', 'ADV', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'DET', 'NOUN', 'NOUN', 'VERB', 'NUM', 'NOUN', 'PRT', 'NUM')\n",
      "Sentence: wednesday 's volume on the first section was estimated *-1 at 900 million shares in line with tuesday 's 909 million\n",
      "Predicted tags: ['NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN']\n",
      "Actual tags:    ('NOUN', 'PRT', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'VERB', 'VERB', 'X', 'ADP', 'NUM', 'NUM', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NOUN', 'PRT', 'NUM', 'NUM')\n",
      "Sentence: declining issues slightly outnumbered advancing issues 454 to 451\n",
      "Predicted tags: ['VERB', 'NOUN', 'ADV', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN']\n",
      "Actual tags:    ('VERB', 'NOUN', 'ADV', 'VERB', 'VERB', 'NOUN', 'NUM', 'PRT', 'NUM')\n",
      "Sentence: investors switched trading focus quickly as they did tuesday *-1 reflecting uncertainty about long-term commitments to any issue or sector traders said 0 *t*-2\n",
      "Predicted tags: ['NOUN', 'VERB', 'NOUN', 'VERB', 'ADV', 'ADP', 'PRON', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN']\n",
      "Actual tags:    ('NOUN', 'VERB', 'NOUN', 'NOUN', 'ADV', 'ADP', 'PRON', 'VERB', 'NOUN', 'X', 'VERB', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'PRT', 'DET', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'VERB', 'X', 'X')\n",
      "Accuracy: 0.4668\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_hmm_tagger(test_data, hmm_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4668\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs_178",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
