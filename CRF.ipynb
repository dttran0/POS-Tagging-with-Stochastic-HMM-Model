{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951a5a27-4605-4f86-a02c-02915a4a2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "from nltk.corpus import treebank, brown, conll2000, reuters\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be437517-cbd0-44a7-8471-316d35e2f480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\blkeu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\blkeu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\blkeu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\blkeu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\blkeu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('treebank')\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('reuters')\n",
    "tagged_treebank = treebank.tagged_sents()\n",
    "tagged_brown = brown.tagged_sents()\n",
    "tagged_conll2000 = conll2000.tagged_sents()\n",
    "reuters_files = reuters.fileids()\n",
    "\n",
    "# Tokenize the raw text into sentences and apply POS tagging\n",
    "tagged_sentences_reuters = []\n",
    "for file_id in reuters_files:\n",
    "    raw_text = reuters.raw(file_id)\n",
    "    sentences = sent_tokenize(raw_text)\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        tagged_sentence = pos_tag(words, tagset='universal')\n",
    "        tagged_sentences_reuters.append(tagged_sentence)\n",
    "        \n",
    "# print(treebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12d24fb-9770-40f0-82b2-acdc5e7e183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(sentence, i):\n",
    "    word = sentence[i][0]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'is_first': i == 0, #if the word is a first word\n",
    "        'is_last': i == len(sentence) - 1,  #if the word is a last word\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,      #word is in uppercase\n",
    "        'is_all_lower': word.lower() == word,      #word is in lowercase\n",
    "         #prefix of the word\n",
    "        'prefix-1': word[0],   \n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "         #suffix of the word\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "         #extracting previous word\n",
    "        'prev_word': '' if i == 0 else sentence[i-1][0],\n",
    "         #extracting next word\n",
    "        'next_word': '' if i == len(sentence)-1 else sentence[i+1][0],\n",
    "        'has_hyphen': '-' in word,    #if word has hypen\n",
    "        'is_numeric': word.isdigit(),  #if word is in numeric\n",
    "        'capitals_inside': word[1:].lower() != word[1:]\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2419db-d8c0-4508-b04f-11fd5838edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract(corpus):\n",
    "    X = []\n",
    "    y = []\n",
    "    for sentence in corpus:\n",
    "    \tX_sentence = []\n",
    "    \ty_sentence = []\n",
    "    \tfor i in range(len(sentence)):\n",
    "    \t\tX_sentence.append(word_features(sentence, i))\n",
    "    \t\ty_sentence.append(sentence[i][1])\n",
    "    \tX.append(X_sentence)\n",
    "    \ty.append(y_sentence)\n",
    "    \n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    split = int(0.8 * len(X))\n",
    "    X_train = X[:split]\n",
    "    y_train = y[:split]\n",
    "    X_test = X[split:]\n",
    "    y_test = y[split:]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65141a4f-6841-45eb-9d5d-3ac918465c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_pred(X_train, y_train, X_test, y_test):\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "    \talgorithm='lbfgs',\n",
    "    \tc1=0.1,\n",
    "    \tc2=0.1,\n",
    "    \tmax_iterations=100,\n",
    "    \tall_possible_transitions=True\n",
    "    )\n",
    "    crf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data and evaluate the performance\n",
    "    y_pred = crf.predict(X_test)\n",
    "    \n",
    "    print(f\"Accuracy: {metrics.flat_accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0125ec82-f6e9-483c-b35c-5d01d6631630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(corpus):\n",
    "    print(f\"CRF {corpus}\\n\\n\")\n",
    "    X_train, y_train, X_test, y_test = feature_extract(corpus)\n",
    "    train_and_pred(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fddc0d70-bf76-42a2-9879-c4cfe5d546b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF [[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]\n",
      "\n",
      "\n",
      "Accuracy: 0.9632716203403363\n",
      "CRF [[('Confidence', 'NN'), ('in', 'IN'), ('the', 'DT'), ('pound', 'NN'), ('is', 'VBZ'), ('widely', 'RB'), ('expected', 'VBN'), ('to', 'TO'), ('take', 'VB'), ('another', 'DT'), ('sharp', 'JJ'), ('dive', 'NN'), ('if', 'IN'), ('trade', 'NN'), ('figures', 'NNS'), ('for', 'IN'), ('September', 'NNP'), (',', ','), ('due', 'JJ'), ('for', 'IN'), ('release', 'NN'), ('tomorrow', 'NN'), (',', ','), ('fail', 'VB'), ('to', 'TO'), ('show', 'VB'), ('a', 'DT'), ('substantial', 'JJ'), ('improvement', 'NN'), ('from', 'IN'), ('July', 'NNP'), ('and', 'CC'), ('August', 'NNP'), (\"'s\", 'POS'), ('near-record', 'JJ'), ('deficits', 'NNS'), ('.', '.')], [('Chancellor', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Exchequer', 'NNP'), ('Nigel', 'NNP'), ('Lawson', 'NNP'), (\"'s\", 'POS'), ('restated', 'VBN'), ('commitment', 'NN'), ('to', 'TO'), ('a', 'DT'), ('firm', 'NN'), ('monetary', 'JJ'), ('policy', 'NN'), ('has', 'VBZ'), ('helped', 'VBN'), ('to', 'TO'), ('prevent', 'VB'), ('a', 'DT'), ('freefall', 'NN'), ('in', 'IN'), ('sterling', 'NN'), ('over', 'IN'), ('the', 'DT'), ('past', 'JJ'), ('week', 'NN'), ('.', '.')], ...]\n",
      "\n",
      "\n",
      "Accuracy: 0.9780065692168802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9832842396121162\n"
     ]
    }
   ],
   "source": [
    "pipeline(tagged_treebank)\n",
    "# pipeline(tagged_brown)\n",
    "pipeline(tagged_conll2000)\n",
    "pipeline(tagged_sentences_reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f222ec17-63e4-47cb-82fc-4022db8497d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF [[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pipeline(tagged_brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2091cd83-b343-4d3a-a242-88dcce1e7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^ 5:20PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae8e2863-702c-4085-a546-a65c20b0afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pycrfsuite\n",
    "\n",
    "# # Train a CRF model suing pysrfsuite\n",
    "# trainer = pycrfsuite.Trainer(verbose=False)\n",
    "# for x, y in zip(X_train, y_train):\n",
    "# \ttrainer.append(x, y)\n",
    "# trainer.set_params({\n",
    "# \t'c1': 1.0,\n",
    "# \t'c2': 1e-3,\n",
    "# \t'max_iterations': 50,\n",
    "# \t'feature.possible_transitions': True\n",
    "# })\n",
    "# trainer.train('pos.crfsuite')\n",
    "\n",
    "# # Tag a new sentence\n",
    "# tagger = pycrfsuite.Tagger()\n",
    "# tagger.open('pos.crfsuite')\n",
    "# sentence = 'Geeksforgeeks is a best platform for students.'.split()\n",
    "# features = [word_features(sentence, i) for i in range(len(sentence))]\n",
    "# tags = tagger.tag(features)\n",
    "# print(list(zip(sentence, tags)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c379b-38d2-47f1-a8e1-ef4fa4010bab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
